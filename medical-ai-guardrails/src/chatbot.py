import os
import sys
from typing import Optional, Dict, Any
import json
from dotenv import load_dotenv

load_dotenv()

DISCLAIMER = (
    "Disclaimer: This chatbot is designed solely to provide general educational information "
    "related to health and medicine. It does not constitute, and should not be construed as, "
    "professional medical advice, diagnosis, or treatment. "
    "The chatbot is not a substitute for consultation with qualified healthcare professionals "
    "and must not be relied upon for decisions regarding your health or medical care. "
    "It does not provide emergency assistance, personalized treatment recommendations, "
    "or individualized medication dosages. "
    "If you are experiencing a medical emergency, please immediately contact your local "
    "emergency number or go to the nearest hospital. For non-emergency concerns, you should "
    "always seek advice from a licensed physician or other qualified medical provider. "
    "The information and responses generated by this system are for general informational "
    "purposes only and are not guaranteed to be accurate, comprehensive, or up to date. "
    "The developers and operators of this chatbot disclaim any and all responsibility or "
    "liability arising from reliance on the chatbotâ€™s outputs."
)

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
GUARDRAILS_DIR = os.path.join(SCRIPT_DIR, "guardrails")
INPUT_GUARDS_DIR = os.path.join(GUARDRAILS_DIR, "input_guards")
OUTPUT_GUARDS_DIR = os.path.join(GUARDRAILS_DIR, "output_guards")

if SCRIPT_DIR not in sys.path:
    sys.path.insert(0, SCRIPT_DIR)
if GUARDRAILS_DIR not in sys.path:
    sys.path.insert(0, GUARDRAILS_DIR)
if INPUT_GUARDS_DIR not in sys.path:
    sys.path.insert(0, INPUT_GUARDS_DIR)
if OUTPUT_GUARDS_DIR not in sys.path:
    sys.path.insert(0, OUTPUT_GUARDS_DIR)

try:
    from guardrails.input_guards.emergency_detector import EmergencyDetector
    from guardrails.input_guards.suicide_detector import (
        OneProtoSuicideDetector,
        load_csv,
        fit_one_prototype,
    )
    from guardrails.input_guards.injection_detector import InjectionDetector
    from guardrails.input_guards.privacy_guard import MedicalScopeDetector
    from guardrails.output_guards.privacy_guard_output import privacy_guard_output, OutputPrivacyGuard
except ImportError:
    print("Error importing detectors. Please verify guardrails modules are available.")
    sys.exit(1)


class MedicalChatbot:
    """Medical AI chatbot with safety guards."""

    def __init__(
        self,
        llm_api_key: str,
        llm_model: str = "gpt-4o-mini",
        project_root: Optional[str] = None,
        verbose: bool = False,
    ):
        self.llm_api_key = llm_api_key
        self.llm_model = llm_model
        self.verbose = verbose

        self.project_root = SCRIPT_DIR if project_root is None else project_root
        self.input_guards_dir = os.path.join(self.project_root, "guardrails", "input_guards")
        self.output_guards_dir = os.path.join(self.project_root, "guardrails", "output_guards")
        self.output_artifacts_dir = os.path.join(self.output_guards_dir, "output_proto_artifacts")

        self.emergency_keywords_path = os.path.join(self.input_guards_dir, "emergency_keywords.json")
        self.suicide_csv_path = os.path.join(self.input_guards_dir, "Suicide_Detection.csv")
        self.injection_parquet_path = os.path.join(self.input_guards_dir, "injection_dataset.parquet")
        self.injection_artifacts_dir = os.path.join(self.input_guards_dir, "injection_proto_artifacts")
        self.suicide_artifacts_dir = os.path.join(self.input_guards_dir, "suicide_proto_artifacts")

        self._log("=" * 70)
        self._log("Initializing Medical Chatbot Safety Guards")
        self._log("=" * 70)
        self._log(f"Project root: {self.project_root}")
        self._log(f"Input guards: {self.input_guards_dir}")

        self._init_emergency_detector()
        self._init_suicide_detector()
        self._init_injection_detector()
        self._init_privacy_guards()

        self._log("\nAll safety guards initialized successfully.")
        self._log("=" * 70)

    def _log(self, *args, **kwargs):
        if self.verbose:
            print(*args, **kwargs)

    def _init_emergency_detector(self):
        self._log("\n[1/5] Loading Emergency Detector...")
        if os.path.exists(self.emergency_keywords_path):
            self._log(f"Using keywords file: {self.emergency_keywords_path}")
            self.emergency_detector = EmergencyDetector(self.emergency_keywords_path)
            self._log(f"Loaded {len(self.emergency_detector.keywords)} emergency keywords")
        else:
            self._log(f"Missing file: {self.emergency_keywords_path}")
            raise FileNotFoundError("Emergency keywords file missing.")

    def _init_suicide_detector(self):
        self._log("\n[2/5] Loading Suicide Risk Detector...")
        proto_path = os.path.join(self.suicide_artifacts_dir, "suicide_proto.pt")
        meta_path = os.path.join(self.suicide_artifacts_dir, "meta.json")

        if os.path.exists(proto_path) and os.path.exists(meta_path):
            import torch
            from sentence_transformers import SentenceTransformer

            with open(meta_path, "r") as f:
                meta = json.load(f)

            model = SentenceTransformer(meta["model_name"])
            proto_data = torch.load(proto_path, map_location="cpu", weights_only=False)
            suicide_proto = proto_data["suicide_proto"]
            threshold = meta["threshold"]

            self.suicide_detector = OneProtoSuicideDetector(suicide_proto, threshold, model)
            self._log(f"Loaded pre-trained model (threshold={threshold:.4f})")
        elif os.path.exists(self.suicide_csv_path):
            self._log("No pre-trained model found.")
            self._log(f"Training data available: {self.suicide_csv_path}")
            self._log(f"python {os.path.join(self.input_guards_dir, 'suicide_detector.py')}")
            self._log("Suicide detection is disabled for now.")
            self.suicide_detector = None
        else:
            self._log(f"No model or training data found. Expected CSV: {self.suicide_csv_path}")
            self._log("Suicide detection is disabled.")
            self.suicide_detector = None

    def _init_injection_detector(self):
        self._log("\n[3/5] Loading Injection Detector...")
        if os.path.exists(self.injection_artifacts_dir):
            proto_path = os.path.join(self.injection_artifacts_dir, "injection_proto.pt")
            meta_path = os.path.join(self.injection_artifacts_dir, "meta.json")

            if os.path.exists(proto_path) and os.path.exists(meta_path):
                self._log(f"Loading pre-trained model from: {self.injection_artifacts_dir}")
                self.injection_detector = InjectionDetector.load_from_artifacts(self.injection_artifacts_dir)
                self._log("Pre-trained injection detector loaded")
            else:
                self._log(f"Artifacts incomplete in: {self.injection_artifacts_dir}")
                self._handle_missing_injection_model()
        else:
            self._log(f"No pre-trained model found: {self.injection_artifacts_dir}")
            self._handle_missing_injection_model()

    def _handle_missing_injection_model(self):
        if os.path.exists(self.injection_parquet_path):
            self._log(f"Training data available: {self.injection_parquet_path}")
            self._log(f"python {os.path.join(self.input_guards_dir, 'injection_detector.py')} train")
        else:
            self._log(f"No training data found: {self.injection_parquet_path}")
        self._log("Injection detection is disabled for now.")
        self.injection_detector = None

    def _init_privacy_guards(self):
        self._log("\n[4/5] Loading Privacy Guards...")
        self.privacy_input = MedicalScopeDetector()
        self._log("Input privacy guard loaded")

        self._log("\n[5/5] Loading Output Privacy Guard (prototype)...")
        os.makedirs(self.output_artifacts_dir, exist_ok=True)
        pt_path = os.path.join(self.output_artifacts_dir, "scope_prototypes.pt")
        npz_path = os.path.join(self.output_artifacts_dir, "scope_prototypes.npz")
        proto_path = pt_path if os.path.exists(pt_path) else (npz_path if os.path.exists(npz_path) else None)

        if proto_path:
            self.privacy_output = OutputPrivacyGuard(proto_path=proto_path)
            self._log(f"Output privacy guard loaded from: {proto_path}")
        else:
            self._log("No output prototypes found; using built-in seeds (fallback).")
            self.privacy_output = OutputPrivacyGuard()

        self._log("Output privacy guard loaded")

    def _call_llm_api(self, user_message: str) -> str:
        import requests
        import os

        api_key = self.llm_api_key or os.getenv("OPENROUTER_API_KEY")
        if not api_key or api_key.startswith("your-api-key"):
            return (
                "No API key found. Please set OPENROUTER_API_KEY "
                "environment variable with your OpenRouter key (sk-or-...)."
            )

        if api_key.startswith("sk-or-"):
            api_url = "https://openrouter.ai/api/v1/chat/completions"
            model_name = "meta-llama/llama-3.3-70b-instruct:free"
        else:
            return "Unsupported API key type. Please use an OpenRouter key (it should start with sk-or-)."

        headers = {
            "Authorization": f"Bearer {api_key}",
            "HTTP-Referer": "https://openrouter.ai",
            "X-Title": "Medical-AI-Guardrails",
        }

        system_prompt = (
            "You are a helpful medical information assistant. "
            "You provide general medical information and education. "
            "You must not provide personalized medical advice, diagnoses, "
            "or treatment recommendations. Always remind users to consult "
            "healthcare professionals for personal medical concerns."
        )

        payload = {
            "model": model_name,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message},
            ],
            "max_tokens": 500,
            "temperature": 0.7,
        }

        try:
            response = requests.post(api_url, headers=headers, json=payload, timeout=60)
            response.raise_for_status()
            data = response.json()
            return data["choices"][0]["message"]["content"]
        except requests.exceptions.HTTPError as e:
            if self.verbose:
                return f"OpenRouter HTTP Error: {e.response.text}"
            return "Upstream model request failed. Please try again later."
        except Exception as e:
            if self.verbose:
                print(f"OpenRouter API Error: {e}")
            return "Unable to connect to the model right now. Please check your OpenRouter API key or try again later."

    def chat(self, user_input: str) -> Dict[str, Any]:
        if self.verbose:
            print("\n" + "=" * 70)
            print(f"Processing: {user_input[:50]}...")
            print("=" * 70)

        if self.verbose:
            print("\n[Guard 1/4] Checking for emergencies...")
        emergency_warning = self.emergency_detector.check(user_input)
        if emergency_warning:
            if self.verbose:
                print("Emergency detected")
            return {
                "response": emergency_warning,
                "blocked_by": "emergency_detector",
                "warning": "Medical emergency detected",
                "safe": False,
            }

        if self.verbose:
            print("No emergency detected")

        if self.verbose:
            print("\n[Guard 2/4] Checking for suicide risk...")
        if self.suicide_detector:
            suicide_warning = self.suicide_detector.check(user_input)
            if suicide_warning:
                if self.verbose:
                    print("Suicide risk detected")
                return {
                    "response": suicide_warning,
                    "blocked_by": "suicide_detector",
                    "warning": "Suicide risk detected",
                    "safe": False,
                }
            if self.verbose:
                print("No suicide risk detected")
        else:
            if self.verbose:
                print("Suicide detector not available (skipped)")

        if self.verbose:
            print("\n[Guard 3/4] Checking for prompt injection...")
        if self.injection_detector:
            injection_warning = self.injection_detector.check(user_input, verbose=self.verbose)
            if injection_warning:
                if self.verbose:
                    print("Prompt injection detected")
                return {
                    "response": injection_warning,
                    "blocked_by": "injection_detector",
                    "warning": "Prompt injection detected",
                    "safe": False,
                }
            if self.verbose:
                print("No prompt injection detected")
        else:
            if self.verbose:
                print("Injection detector not available (skipped)")

        if self.verbose:
            print("\n[Guard 4/4] Checking for personalized medical requests...")
        privacy_warning = self.privacy_input.check(user_input)
        if privacy_warning:
            if self.verbose:
                print("Personalized request blocked")
            return {
                "response": privacy_warning,
                "blocked_by": "privacy_input",
                "warning": "Personalized medical advice requested",
                "safe": False,
            }

        if self.verbose:
            print("Input passed all guards")

        if self.verbose:
            print("\n[LLM] Generating response...")
        try:
            llm_response = self._call_llm_api(user_input)
            if self.verbose:
                print(f"Response generated ({len(llm_response)} chars)")
        except Exception as e:
            if self.verbose:
                print(f"LLM Error: {e}")
            return {
                "response": "I am having trouble right now. Please try again.",
                "blocked_by": "llm_error",
                "warning": str(e),
                "safe": False,
            }

        if self.verbose:
            print("\n[Output Guard] Validating response...")
        is_safe, output_warning = self.privacy_output.check(llm_response)
        if not is_safe:
            if self.verbose:
                print("Output blocked")
                print(f"Reason: {output_warning}")
            return {
                "response": (
                    "I generated a response that may contain personalized medical advice. "
                    "For your safety, I cannot provide this information. "
                    "Please consult a licensed healthcare professional for personalized guidance."
                ),
                "blocked_by": "privacy_output",
                "warning": output_warning,
                "safe": False,
            }

        if self.verbose:
            print("Output validated - safe to return")
            print("\nAll checks passed - returning response")
            print("=" * 70)

        return {
            "response": llm_response,
            "blocked_by": None,
            "warning": None,
            "safe": True,
        }

    def interactive_mode(self):
        print("\n" + "=" * 70)
        print("Medical AI Chatbot")
        print("=" * 70)
        print(DISCLAIMER)
        print("=" * 70)
        print("Type 'quit' or 'exit' to stop")
        print("Type 'verbose' to toggle developer logging")
        print("=" * 70)

        while True:
            try:
                user_input = input("\nYou: ").strip()
                if not user_input:
                    continue
                if user_input.lower() in ["quit", "exit", "bye"]:
                    print("\nGoodbye!")
                    break
                if user_input.lower() == "verbose":
                    self.verbose = not self.verbose
                    print(f"Verbose mode: {'ON' if self.verbose else 'OFF'}")
                    continue

                result = self.chat(user_input)
                print("\nBot:", result["response"])

                if not result["safe"] and self.verbose:
                    print(f"\n[Debug] Blocked by: {result['blocked_by']}")
                    print(f"[Debug] Reason: {result['warning']}")
            except KeyboardInterrupt:
                print("\n\nGoodbye!")
                break
            except Exception:
                print("\nAn unexpected error occurred.")
                if self.verbose:
                    import traceback
                    traceback.print_exc()


def main():
    llm_api_key = os.getenv("OPENROUTER_API_KEY")
    llm_model = os.getenv("DEFAULT_MODEL", "meta-llama/llama-3.3-70b-instruct:free")
    verbose_flag = os.getenv("VERBOSE", "False").lower() == "true"

    project_root = os.path.dirname(os.path.abspath(__file__))
    if not llm_api_key:
        print("Warning: No OpenRouter API key found.")
        print("Set OPENROUTER_API_KEY in your .env (sk-or-...)")
        print("Chatbot will use fallback responses\n")

    try:
        chatbot = MedicalChatbot(
            llm_api_key=llm_api_key,
            llm_model=llm_model,
            project_root=project_root,
            verbose=verbose_flag,
        )
        chatbot.interactive_mode()
    except Exception:
        print("Failed to initialize. Please try again or contact the administrator.")
        if verbose_flag:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
