import os
import sys
import json
import logging
import torch
from typing import Optional, Dict, Any
from dotenv import load_dotenv
import sentence_transformers as _st_mod

load_dotenv()

# === Logging ===
LOG_FILE = os.path.join(os.path.dirname(__file__), "medbot.log")
logging.basicConfig(
    filename=LOG_FILE,
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    encoding="utf-8",
)

# === SentenceTransformer cache wrapper ===
_OrigST = _st_mod.SentenceTransformer
_ST_CACHE = {}

def _canon(name: str) -> str:
    name = (name or "").strip()
    return name if name.startswith("sentence-transformers/") else f"sentence-transformers/{name}"

def _CachedST(model_name, *args, **kwargs):
    key = _canon(model_name)
    if key in _ST_CACHE:
        return _ST_CACHE[key]
    short = key.split("/", 1)[1]
    if short in _ST_CACHE:
        return _ST_CACHE[short]
    obj = _OrigST(key, *args, **kwargs)
    _ST_CACHE[key] = _ST_CACHE[short] = obj
    return obj

_st_mod.SentenceTransformer = _CachedST

_MODEL_CACHE: Dict[str, object] = {}

def get_cached_model(model_name: str):
    from sentence_transformers import SentenceTransformer
    can = _canon(model_name)
    short = can.split("/", 1)[1]
    if can in _MODEL_CACHE:
        return _MODEL_CACHE[can]
    if short in _MODEL_CACHE:
        return _MODEL_CACHE[short]
    m = SentenceTransformer(can)
    _MODEL_CACHE[can] = _MODEL_CACHE[short] = m
    return m


# === Constants ===
DISCLAIMER = ( "Disclaimer: This chatbot is designed solely to provide general educational information " 
              "related to health and medicine. It does not constitute, and should not be construed as, " 
              "professional medical advice, diagnosis, or treatment. " 
              "The chatbot is not a substitute for consultation with qualified healthcare professionals " 
              "and must not be relied upon for decisions regarding your health or medical care. " 
              "It does not provide emergency assistance, personalized treatment recommendations, " 
              "or individualized medication dosages. " 
              "If you are experiencing a medical emergency, please immediately contact your local " 
              "emergency number or go to the nearest hospital. For non-emergency concerns, you should " 
              "always seek advice from a licensed physician or other qualified medical provider. " 
              "The information and responses generated by this system are for general informational " 
              "purposes only and are not guaranteed to be accurate, comprehensive, or up to date. " 
              "The developers and operators of this chatbot disclaim any and all responsibility or " 
              "liability arising from reliance on the chatbotâ€™s outputs." )

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
GUARDRAILS_DIR = os.path.join(SCRIPT_DIR, "guardrails")
INPUT_GUARDS_DIR = os.path.join(GUARDRAILS_DIR, "input_guards")
OUTPUT_GUARDS_DIR = os.path.join(GUARDRAILS_DIR, "output_guards")

for p in [SCRIPT_DIR, GUARDRAILS_DIR, INPUT_GUARDS_DIR, OUTPUT_GUARDS_DIR]:
    if p not in sys.path:
        sys.path.insert(0, p)

# === Import safety guards ===
try:
    from guardrails.input_guards.emergency_detector import EmergencyDetector
    from guardrails.input_guards.suicide_detector import OneProtoSuicideDetector
    from guardrails.input_guards.injection_detector import InjectionDetector
    from guardrails.input_guards.privacy_guard import MedicalScopeDetector
    from guardrails.output_guards.privacy_guard_output import OutputPrivacyGuard
except ImportError:
    print("Error importing guard modules.")
    sys.exit(1)


# === Main Chatbot ===
class MedicalChatbot:
    """Medical chatbot with safety guardrails."""

    def __init__(
        self,
        llm_api_key: str,
        llm_model: str = "gpt-4o-mini",
        project_root: Optional[str] = None,
        verbose: bool = False,
    ):
        self.llm_api_key = llm_api_key
        self.llm_model = llm_model
        self.verbose = verbose

        self.project_root = project_root or SCRIPT_DIR
        self.input_guards_dir = os.path.join(self.project_root, "guardrails", "input_guards")
        self.output_guards_dir = os.path.join(self.project_root, "guardrails", "output_guards")
        self.output_artifacts_dir = os.path.join(self.output_guards_dir, "output_proto_artifacts")

        self.emergency_keywords_path = os.path.join(self.input_guards_dir, "emergency_keywords.json")
        self.suicide_artifacts_dir = os.path.join(self.input_guards_dir, "suicide_proto_artifacts")
        self.injection_artifacts_dir = os.path.join(self.input_guards_dir, "injection_proto_artifacts")

        self._log("=" * 70)
        self._log("Initializing Safety Modules")
        self._log("=" * 70)

        self._init_emergency_detector()
        self._init_suicide_detector()
        self._init_injection_detector()
        self._init_privacy_guards()

        self._log("\nAll safety modules loaded successfully.")
        self._log("=" * 70)

    def _log(self, *args, **kwargs):
        if self.verbose:
            print(*args, **kwargs)

    # === Guard initializers ===
    def _init_emergency_detector(self):
        self._log("\n[1/5] Emergency Detector")
        if not os.path.exists(self.emergency_keywords_path):
            raise FileNotFoundError("Missing emergency_keywords.json")
        self.emergency_detector = EmergencyDetector(self.emergency_keywords_path)
        self._log(f"Loaded {len(self.emergency_detector.keywords)} emergency keywords")

    def _init_suicide_detector(self):
        self._log("\n[2/5] Suicide Risk Detector")
        proto = os.path.join(self.suicide_artifacts_dir, "suicide_proto.pt")
        meta = os.path.join(self.suicide_artifacts_dir, "meta.json")
        if not (os.path.exists(proto) and os.path.exists(meta)):
            raise FileNotFoundError("Missing suicide artifacts")

        with open(meta, "r", encoding="utf-8") as f:
            meta_data = json.load(f)

        model = get_cached_model(meta_data.get("model_name", "all-mpnet-base-v2"))
        proto_data = torch.load(proto, map_location="cpu", weights_only=True)
        threshold = float(meta_data["threshold"])
        self.suicide_detector = OneProtoSuicideDetector(proto_data["suicide_proto"], threshold, model)
        self._log(f"Loaded suicide model (threshold={threshold:.4f})")

    def _init_injection_detector(self):
        self._log("\n[3/5] Injection Detector")
        if not os.path.isdir(self.injection_artifacts_dir):
            raise FileNotFoundError("Missing injection artifacts directory")
        self.injection_detector = InjectionDetector.load_from_artifacts(self.injection_artifacts_dir)
        self._log("Injection detector loaded")

    def _init_privacy_guards(self):
        self._log("\n[4/5] Privacy Guards")
        self.privacy_input = MedicalScopeDetector()
        os.makedirs(self.output_artifacts_dir, exist_ok=True)
        proto_path = os.path.join(self.output_artifacts_dir, "scope_prototypes.pt")

        try:
            blob = torch.load(proto_path, map_location="cpu", weights_only=False)
            self._log(f"Loaded output prototypes: {list(blob.keys())}")
        except Exception:
            logging.exception("Output prototype load failed")

        model = get_cached_model("sentence-transformers/all-MiniLM-L6-v2")
        self.privacy_output = OutputPrivacyGuard(proto_path=proto_path, model=model, forbid_fallback=True)
        self._log("Privacy guards initialized")

    # === LLM call ===
    def _call_llm_api(self, user_message: str) -> str:
        import requests
        api_key = self.llm_api_key or os.getenv("OPENROUTER_API_KEY")
        if not api_key or not api_key.startswith("sk-or-"):
            return "Missing or invalid OpenRouter API key."

        api_url = "https://openrouter.ai/api/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {api_key}",
            "HTTP-Referer": "https://openrouter.ai",
            "X-Title": "Medical-AI-Guardrails",
        }

        payload = {
            "model": "meta-llama/llama-3.3-70b-instruct:free",
            "messages": [
                {"role": "system", "content": "You are a safe medical info assistant."},
                {"role": "user", "content": user_message},
            ],
            "max_tokens": 500,
            "temperature": 0.7,
        }

        try:
            r = requests.post(api_url, headers=headers, json=payload, timeout=60)
            r.raise_for_status()
            return r.json()["choices"][0]["message"]["content"]
        except Exception as e:
            logging.exception("OpenRouter API error")
            return f"Model request failed: {e}"

    # === Main chat pipeline ===
    def chat(self, user_input: str) -> Dict[str, Any]:
        if self.verbose:
            print(f"\nProcessing: {user_input[:60]}...")

        # 1. Emergency
        msg = self.emergency_detector.check(user_input)
        if msg:
            return {"response": msg, "blocked_by": "emergency", "safe": False}

        # 2. Suicide risk
        if self.suicide_detector:
            msg = self.suicide_detector.check(user_input)
            if msg:
                return {"response": msg, "blocked_by": "suicide", "safe": False}

        # 3. Prompt injection
        if self.injection_detector:
            msg = self.injection_detector.check(user_input, verbose=self.verbose)
            if msg:
                return {"response": msg, "blocked_by": "injection", "safe": False}

        # 4. Privacy input
        msg = self.privacy_input.check(user_input)
        if msg:
            return {"response": msg, "blocked_by": "privacy_input", "safe": False}

        # Generate
        llm_output = self._call_llm_api(user_input)

        # Validate output
        is_safe, warn = self.privacy_output.check(llm_output)
        if not is_safe:
            return {
                "response": (
                    "This message might include personal medical advice. "
                    "Please consult a healthcare professional."
                ),
                "blocked_by": "privacy_output",
                "warning": warn,
                "safe": False,
            }

        return {"response": llm_output, "blocked_by": None, "safe": True}

    # === Interactive mode ===
    def interactive_mode(self):
        print("\n" + "=" * 70)
        print("Medical AI Chatbot")
        print("=" * 70)
        print(DISCLAIMER)
        print("=" * 70)
        print("Type 'quit' to exit or 'verbose' to toggle logs.")
        print("=" * 70)

        while True:
            try:
                user_input = input("\nYou: ").strip()
                if not user_input:
                    continue
                if user_input.lower() in ["quit", "exit"]:
                    print("\nGoodbye!")
                    break
                if user_input.lower() == "verbose":
                    self.verbose = not self.verbose
                    print(f"Verbose mode: {'ON' if self.verbose else 'OFF'}")
                    continue

                result = self.chat(user_input)
                print("\nBot:", result["response"])

                if not result["safe"] and self.verbose:
                    print(f"[Debug] Blocked by: {result['blocked_by']}")
            except KeyboardInterrupt:
                print("\n\nGoodbye!")
                break
            except Exception:
                print("Unexpected error.")
                if self.verbose:
                    import traceback
                    traceback.print_exc()


# === Entry point ===
def main():
    llm_api_key = os.getenv("OPENROUTER_API_KEY")
    llm_model = os.getenv("DEFAULT_MODEL", "meta-llama/llama-3.3-70b-instruct:free")
    verbose = os.getenv("VERBOSE", "False").lower() == "true"

    try:
        bot = MedicalChatbot(llm_api_key, llm_model, verbose=verbose)
        bot.interactive_mode()
    except Exception:
        print("Initialization failed. Check logs for details.")
        logging.exception("Startup error")
        sys.exit(1)


if __name__ == "__main__":
    main()
